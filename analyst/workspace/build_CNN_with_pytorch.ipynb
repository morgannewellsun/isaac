{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da96116-06fe-42b8-8c3c-1a4605855239",
   "metadata": {},
   "source": [
    "Build CNN Classifier using Simulation data and test it using real images\n",
    "=============================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57618d6-de3f-4c90-bbe7-fb6f18055647",
   "metadata": {},
   "source": [
    "Get Train and Test data using the simulation\n",
    "----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e494d06-695e-4c9a-8518-a056fadf53aa",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "This section of the tutorial assumes that you have a functional ISAAC simulation working.\n",
    "If not, you'll have to use pre-generated data for this tutorial purpose (not recommended).\n",
    "</div>\n",
    "\n",
    "Start the simulation:\n",
    "\n",
    "`roslaunch isaac sim.launch gzclient:=true pose:=\"10.5 -8 4.5 0 0 0 1\"`\n",
    "\n",
    "Spawn the object\n",
    "\n",
    "`roslaunch isaac_gazebo spawn_object.launch spawn:=sock`\n",
    "\n",
    "Add an object to the simulation to be the anomaly (in this case a sock):\n",
    "\n",
    "`rosrun img_analysis get_train_data -path_dataset $PATH_DATASET -vent_poses $VENT_POSES -other_poses $OTHER_POSES [OPTIONS]`\n",
    "\n",
    "Arguments:\n",
    " - `path_dataset`        - Path to where to save the datasets, mandatory to define.\n",
    " - `vent_poses`          - *.txt* file containing the vent poses\n",
    " - `other_poses`         - *.txt* file containing the other non-vent poses\n",
    " - `robot_dist`          - Robot's distance to vent, standard is 1m\n",
    " - `train_pics_per_vent` - Number of pictures taken per vent/other for train data\n",
    " - `test_pics_per_vent`  - Number of pictures taken per vent/other for test data\n",
    " \n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1486d87-cb5e-499d-9da9-1666665df5c4",
   "metadata": {},
   "source": [
    "Train CNN\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2443f-50cc-49a9-9b4e-2f1018fc6904",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameters\n",
    "data_dir = \"data/vent\"  # specify data path\n",
    "classes = [\"free\", \"obstacle\"]  # specify the image classes\n",
    "num_epochs = 30  # number of epochs to train\n",
    "model_name = \"model_cnn.pt\"  # saved model name\n",
    "trace_model_name = \"traced_model_cnn.pt\"  # saved traced model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097ce74d-b561-4aa4-b899-8301205457a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "\n",
    "\n",
    "# Define transforms for the training data and testing data\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize([256, 256]),\n",
    "        transforms.RandomRotation(30),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Pass transforms in here, then run the next cell to see how the transforms look\n",
    "train_data = datasets.ImageFolder(data_dir + \"/train\", transform=train_transforms)\n",
    "test_data = datasets.ImageFolder(data_dir + \"/test\", transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_data, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7534df6b-0f37-4bce-afa5-b34275ef314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize some of the data-----------------------\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "\n",
    "\n",
    "# obtain one batch of training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()  # convert images to numpy for display\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx + 1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(classes[labels[idx]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83e333-f0c8-49c3-96bb-866b7619e3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use GPU if it's available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = models.densenet121(pretrained=True)\n",
    "\n",
    "# Freeze parameters so we don't backprop through them\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(1024, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256, 3),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Only train the classifier parameters, feature parameters are frozen\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afec4b9-6bbc-42ca-aae0-f9b2a6f3d42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.to(device)\n",
    "\n",
    "epochs = num_epochs\n",
    "steps = 0\n",
    "running_loss = 0\n",
    "print_every = 5\n",
    "test_loss_min = np.Inf\n",
    "for epoch in range(epochs):\n",
    "    for inputs, labels in trainloader:\n",
    "        steps += 1\n",
    "        # Move input and label tensors to the default device\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logps = model.forward(inputs)\n",
    "        loss = criterion(logps, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in testloader:\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "                    logps = model.forward(inputs)\n",
    "                    batch_loss = criterion(logps, labels)\n",
    "\n",
    "                    test_loss += batch_loss.item()\n",
    "\n",
    "                    # Calculate accuracy\n",
    "                    ps = torch.exp(logps)\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "\n",
    "            print(\n",
    "                \"Epoch \",\n",
    "                epoch + 1,\n",
    "                \"/\",\n",
    "                epochs,\n",
    "                \".. \" \"Train loss: \",\n",
    "                running_loss / print_every,\n",
    "                \".. \" \"Test loss: \",\n",
    "                test_loss / len(testloader),\n",
    "                \".. \" \"Test accuracy: \",\n",
    "                accuracy / len(testloader),\n",
    "                \"\",\n",
    "            )\n",
    "            running_loss = 0\n",
    "\n",
    "            # save model if validation loss has decreased\n",
    "            if test_loss <= test_loss_min:\n",
    "                torch.save(model.state_dict(), model_name)\n",
    "                test_loss_min = test_loss\n",
    "\n",
    "                model.to(\"cpu\")\n",
    "                # An example input you would normally provide to your model's forward() method.\n",
    "                example = torch.rand(1, 3, 224, 224)\n",
    "                # Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.\n",
    "                traced_script_module = torch.jit.trace(model, example)\n",
    "                output = traced_script_module(torch.ones(1, 3, 224, 224))\n",
    "                print(output)\n",
    "                traced_script_module.save(trace_model_name)\n",
    "                model.to(device)\n",
    "\n",
    "            model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e03c7-91c9-43ef-9a1c-eb2b468a11dd",
   "metadata": {},
   "source": [
    "Validate Result using Real data\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24eb188-ee78-4de8-a7c8-f1001b0e65ba",
   "metadata": {},
   "source": [
    "To validate the results using real data, let's use an image collected during ISS operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c8f89-22cc-4c2f-8496-bda1e9add48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "# Open and display image\n",
    "image = Image.open(\"data/bags/sock_iss.jpg\")\n",
    "imgplot = plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "# Open model\n",
    "model = models.densenet121(pretrained=True)\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(1024, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(256, 3),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "model.load_state_dict(torch.load(\"model_cnn.pt\"))\n",
    "model.eval()\n",
    "\n",
    "# Classify Image!\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "image_tensor = test_transforms(image).float()\n",
    "image_tensor = image_tensor.unsqueeze_(0)\n",
    "output = model(image_tensor)\n",
    "\n",
    "# Print Result\n",
    "_, predicted = torch.max(output, 1)\n",
    "print(\"Classification: \", classes[predicted])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd58a98-6d15-4c07-8a9a-bd4c40f82d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
